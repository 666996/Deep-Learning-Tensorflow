{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import state_ops\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.framework import constant_op\n",
    "from tensorflow.python.training import optimizer\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class COCOB(optimizer.Optimizer):\n",
    "    def __init__(self, alpha=100, use_locking=False, name='COCOB'):\n",
    "        super(COCOB, self).__init__(use_locking, name)\n",
    "        self._alpha = alpha\n",
    "    \n",
    "    def _create_slots(self, var_list):\n",
    "        for v in var_list:\n",
    "            with ops.colocate_with(v):\n",
    "                gradients_sum = constant_op.constant(0, \n",
    "                                                     shape=v.get_shape(),\n",
    "                                                     dtype=v.dtype.base_dtype)\n",
    "                grad_norm_sum = constant_op.constant(0, \n",
    "                                                     shape=v.get_shape(),\n",
    "                                                     dtype=v.dtype.base_dtype)\n",
    "                L = constant_op.constant(1e-8, shape=v.get_shape(), dtype=v.dtype.base_dtype)\n",
    "                tilde_w = constant_op.constant(0.0, shape=v.get_shape(), dtype=v.dtype.base_dtype)\n",
    "                reward = constant_op.constant(0.0, shape=v.get_shape(), dtype=v.dtype.base_dtype)\n",
    "            self._get_or_make_slot(v, L, \"L\", self._name)\n",
    "            self._get_or_make_slot(v, grad_norm_sum, \"grad_norm_sum\", self._name)\n",
    "            self._get_or_make_slot(v, gradients_sum, \"gradients_sum\", self._name)\n",
    "            self._get_or_make_slot(v, tilde_w, \"tilde_w\", self._name)\n",
    "            self._get_or_make_slot(v, reward, \"reward\", self._name)\n",
    "            \n",
    "    def _apply_dense(self, grad, var):\n",
    "        gradients_sum = self.get_slot(var, \"gradients_sum\")\n",
    "        grad_norm_sum = self.get_slot(var, \"grad_norm_sum\")\n",
    "        tilde_w = self.get_slot(var, \"tilde_w\")\n",
    "        L = self.get_slot(var, \"L\")\n",
    "        reward = self.get_slot(var, \"reward\")\n",
    "        L_update = tf.maximum(L,tf.abs(grad))\n",
    "        gradients_sum_update = gradients_sum + grad\n",
    "        grad_norm_sum_update = grad_norm_sum + tf.abs(grad)\n",
    "        reward_update = tf.maximum(reward-grad*tilde_w,0)\n",
    "        new_w = -gradients_sum_update/(L_update*(tf.maximum(grad_norm_sum_update+L_update,self._alpha*L_update)))*(reward_update+L_update)\n",
    "        var_update = var-tilde_w+new_w\n",
    "        tilde_w_update=new_w\n",
    "        gradients_sum_update_op = state_ops.assign(gradients_sum, gradients_sum_update)\n",
    "        grad_norm_sum_update_op = state_ops.assign(grad_norm_sum, grad_norm_sum_update)\n",
    "        var_update_op = state_ops.assign(var, var_update)\n",
    "        tilde_w_update_op = state_ops.assign(tilde_w, tilde_w_update)\n",
    "        L_update_op = state_ops.assign(L, L_update)\n",
    "        reward_update_op = state_ops.assign(reward, reward_update)\n",
    "        return control_flow_ops.group(*[gradients_sum_update_op,\n",
    "                             var_update_op,\n",
    "                             grad_norm_sum_update_op,\n",
    "                             tilde_w_update_op,\n",
    "                             reward_update_op,\n",
    "                             L_update_op])\n",
    "    \n",
    "    def _apply_sparse(self, grad, var):\n",
    "        return self._apply_dense(grad, var)\n",
    "\n",
    "    def _resource_apply_dense(self, grad, handle):\n",
    "        return self._apply_dense(grad, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting train-images-idx3-ubyte.gz\n",
      "Extracting train-labels-idx1-ubyte.gz\n",
      "Extracting t10k-images-idx3-ubyte.gz\n",
      "Extracting t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('', validation_size = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self,learning_rate=0.01):\n",
    "        self.X = tf.placeholder(tf.float32,shape=[None,784])\n",
    "        self.Y = tf.placeholder(tf.float32,shape=[None,10])\n",
    "        w1 = tf.Variable(tf.random_normal([784,200]))\n",
    "        b1 = tf.Variable(tf.random_normal([200]))\n",
    "        w2 = tf.Variable(tf.random_normal([200,100]))\n",
    "        b2 = tf.Variable(tf.random_normal([100]))\n",
    "        w3 = tf.Variable(tf.random_normal([100,10]))\n",
    "        b3 = tf.Variable(tf.random_normal([10]))\n",
    "        feedforward = tf.nn.relu(tf.matmul(self.X,w1) + b1)\n",
    "        feedforward = tf.nn.relu(tf.matmul(feedforward,w2) + b2)\n",
    "        self.logits = tf.matmul(feedforward,w3) + b3\n",
    "        self.cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels = self.Y, logits = self.logits))\n",
    "        self.optimizer = COCOB().minimize(self.cost)\n",
    "        correct_prediction = tf.equal(tf.argmax(self.logits, 1), tf.argmax(self.Y, 1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epoch = 10\n",
    "\n",
    "train_images = mnist.train.images\n",
    "test_images = mnist.test.images\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "model = Model()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, accuracy train: 0.845753, accuracy testing: 0.886161\n",
      "epoch: 2, accuracy train: 0.939670, accuracy testing: 0.905134\n",
      "epoch: 3, accuracy train: 0.952324, accuracy testing: 0.916295\n",
      "epoch: 4, accuracy train: 0.958951, accuracy testing: 0.925223\n",
      "epoch: 5, accuracy train: 0.963224, accuracy testing: 0.925223\n",
      "epoch: 6, accuracy train: 0.966964, accuracy testing: 0.927455\n",
      "epoch: 7, accuracy train: 0.969685, accuracy testing: 0.933036\n",
      "epoch: 8, accuracy train: 0.971838, accuracy testing: 0.933036\n",
      "epoch: 9, accuracy train: 0.974092, accuracy testing: 0.938616\n",
      "epoch: 10, accuracy train: 0.975728, accuracy testing: 0.940848\n"
     ]
    }
   ],
   "source": [
    "LOSS, ACC_TRAIN, ACC_TEST = [], [], []\n",
    "for i in range(epoch):\n",
    "    total_loss, total_acc = 0, 0\n",
    "    for n in range(0, (mnist.train.images.shape[0] // batch_size) * batch_size, batch_size):\n",
    "        batch_x = train_images[n: n + batch_size,:]\n",
    "        batch_y = np.zeros((batch_size, 10))\n",
    "        batch_y[np.arange(batch_size),mnist.train.labels[n:n+batch_size]] = 1.0\n",
    "        cost, _ = sess.run([model.cost, model.optimizer], \n",
    "                           feed_dict = {model.X : batch_x, \n",
    "                                        model.Y : batch_y})\n",
    "        total_acc += sess.run(model.accuracy, \n",
    "                              feed_dict = {model.X : batch_x, \n",
    "                                           model.Y : batch_y})\n",
    "        total_loss += cost\n",
    "    total_loss /= (mnist.train.images.shape[0] // batch_size)\n",
    "    total_acc /= (mnist.train.images.shape[0] // batch_size)\n",
    "    ACC_TRAIN.append(total_acc)\n",
    "    total_acc = 0\n",
    "    for n in range(0, (mnist.test.images[:1000,:].shape[0] // batch_size) * batch_size, batch_size):\n",
    "        batch_x = test_images[n: n + batch_size,:]\n",
    "        batch_y = np.zeros((batch_size, 10))\n",
    "        batch_y[np.arange(batch_size),mnist.test.labels[n:n+batch_size]] = 1.0\n",
    "        total_acc += sess.run(model.accuracy, \n",
    "                              feed_dict = {model.X : batch_x, \n",
    "                                           model.Y : batch_y})\n",
    "    total_acc /= (mnist.test.images[:1000,:].shape[0] // batch_size)\n",
    "    ACC_TEST.append(total_acc)\n",
    "    print('epoch: %d, accuracy train: %f, accuracy testing: %f'%(i+1, ACC_TRAIN[-1],ACC_TEST[-1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
