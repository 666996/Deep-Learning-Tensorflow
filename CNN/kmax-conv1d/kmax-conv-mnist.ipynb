{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting train-images-idx3-ubyte.gz\n",
      "Extracting train-labels-idx1-ubyte.gz\n",
      "Extracting t10k-images-idx3-ubyte.gz\n",
      "Extracting t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('', one_hot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_conv1d(x, n_filters, kernel_size, strides=1):\n",
    "    return tf.layers.conv1d(inputs = x,\n",
    "                            filters = n_filters,\n",
    "                            kernel_size  = kernel_size,\n",
    "                            strides = strides,\n",
    "                            padding = 'valid',\n",
    "                            use_bias = True,\n",
    "                            activation = tf.nn.relu)\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, learning_rate = 1e-4,\n",
    "                 top_k=5, n_filters=250):\n",
    "        self.n_filters = n_filters\n",
    "        self.kernels = [3, 4, 5]\n",
    "        self.top_k = top_k\n",
    "        self.X = tf.placeholder(tf.float32, [None, 28, 28])\n",
    "        self.Y = tf.placeholder(tf.float32, [None, 10])\n",
    "        parallels = []\n",
    "        for k in self.kernels:\n",
    "            p = add_conv1d(self.X, self.n_filters//len(self.kernels), kernel_size=k)\n",
    "            p = self.add_kmax_pooling(p)\n",
    "            parallels.append(p)\n",
    "        parallels = tf.concat(parallels, axis=-1)\n",
    "        parallels = tf.reshape(parallels, [-1, self.top_k * (len(self.kernels)*(self.n_filters//len(self.kernels)))])\n",
    "        feed = tf.nn.dropout(tf.layers.dense(parallels, self.n_filters, tf.nn.relu), 0.5)\n",
    "        self.logits = tf.layers.dense(parallels, 10)\n",
    "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = self.logits, labels = self.Y))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(self.cost)\n",
    "        self.correct_pred = tf.equal(tf.argmax(self.logits, 1), tf.argmax(self.Y, 1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_pred, tf.float32))\n",
    "        \n",
    "    def add_kmax_pooling(self, x):\n",
    "        Y = tf.transpose(x, [0, 2, 1])\n",
    "        Y = tf.nn.top_k(Y, self.top_k, sorted=False).values\n",
    "        Y = tf.transpose(Y, [0, 2, 1])\n",
    "        return tf.reshape(Y, [-1, self.top_k, self.n_filters//len(self.kernels)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "model = Model()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 10\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, avg loss 1.665750, avg acc 0.576304, time taken 111.642951 secs\n",
      "epoch 2, avg loss 0.911032, avg acc 0.790793, time taken 109.745422 secs\n",
      "epoch 3, avg loss 0.606226, avg acc 0.859648, time taken 109.859211 secs\n",
      "epoch 4, avg loss 0.459514, avg acc 0.890607, time taken 109.979263 secs\n",
      "epoch 5, avg loss 0.374437, avg acc 0.908927, time taken 110.024716 secs\n",
      "epoch 6, avg loss 0.318865, avg acc 0.920873, time taken 109.776753 secs\n",
      "epoch 7, avg loss 0.279638, avg acc 0.930088, time taken 109.840489 secs\n",
      "epoch 8, avg loss 0.250343, avg acc 0.937099, time taken 109.933629 secs\n",
      "epoch 9, avg loss 0.227542, avg acc 0.941962, time taken 109.883575 secs\n",
      "epoch 10, avg loss 0.209218, avg acc 0.945841, time taken 109.611938 secs\n"
     ]
    }
   ],
   "source": [
    "for i in range(EPOCH):\n",
    "    last = time.time()\n",
    "    TOTAL_LOSS, ACCURACY = 0, 0\n",
    "    for n in range(0, (mnist.train.images.shape[0] // BATCH_SIZE) * BATCH_SIZE, BATCH_SIZE):\n",
    "        batch_x = mnist.train.images[n: n + BATCH_SIZE, :].reshape((-1, 28, 28))\n",
    "        acc, cost, _ = sess.run([model.accuracy, model.cost, model.optimizer], \n",
    "                           feed_dict = {model.X : batch_x, \n",
    "                                         model.Y : mnist.train.labels[n: n + BATCH_SIZE, :]})\n",
    "        ACCURACY += acc\n",
    "        TOTAL_LOSS += cost\n",
    "    TOTAL_LOSS /= (mnist.train.images.shape[0] // BATCH_SIZE)\n",
    "    ACCURACY /= (mnist.train.images.shape[0] // BATCH_SIZE)\n",
    "    print('epoch %d, avg loss %f, avg acc %f, time taken %f secs'%(i+1,TOTAL_LOSS,ACCURACY,time.time()-last))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
