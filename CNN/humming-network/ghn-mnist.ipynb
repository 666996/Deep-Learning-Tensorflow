{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting train-images-idx3-ubyte.gz\n",
      "Extracting train-labels-idx1-ubyte.gz\n",
      "Extracting t10k-images-idx3-ubyte.gz\n",
      "Extracting t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('', validation_size = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def differentiable_clip(inputs, alpha, rmin, rmax):\n",
    "    return tf.sigmoid(-alpha * (inputs - rmin)) + tf.sigmoid(alpha * (inputs - rmax))\n",
    "\n",
    "def double_thresholding(inputs, per_pixel=True):\n",
    "    input_shape = inputs.shape.as_list()\n",
    "    if per_pixel:\n",
    "        r = tf.Variable(tf.random_normal(input_shape[1:], stddev=np.sqrt(1/input_shape[-1])))\n",
    "    axis = (1, 2) if len(input_shape) == 4 else (1,)\n",
    "    rmin = tf.reduce_min(inputs, axis=axis, keep_dims=True) * r\n",
    "    rmax = tf.reduce_max(inputs, axis=axis, keep_dims=True) * r\n",
    "    alpha = 0.2\n",
    "    return 0.5 + (inputs - 0.5) * differentiable_clip(inputs, alpha, rmin, rmax)\n",
    "\n",
    "def conv(inputs, filters, kernel_size):\n",
    "    w = tf.Variable(tf.random_normal([kernel_size, kernel_size, int(inputs.shape[-1]), filters], stddev=np.sqrt(1/filters)))\n",
    "    conv = tf.nn.conv2d(inputs,w,strides=[1, 1, 1, 1],padding='VALID')\n",
    "    l = tf.constant(functools.reduce(lambda x, y: x * y, w.shape.as_list()[:3], 1),dtype=tf.float32)\n",
    "    mean_weight = tf.constant(1, shape=[kernel_size, kernel_size, inputs.shape.as_list()[-1], 1],dtype=tf.float32)\n",
    "    mean_x = 1. / l * tf.nn.conv2d(inputs, mean_weight, strides=[1, 1, 1, 1], padding='VALID')\n",
    "    mean_w = tf.reduce_mean(w, axis=(0, 1, 2), keep_dims=True)\n",
    "    hout = (2. / l) * conv - mean_w - mean_x\n",
    "    return double_thresholding(hout)\n",
    "\n",
    "def fully_connected(inputs, out_size):\n",
    "    w = tf.Variable(tf.random_normal([int(inputs.shape[-1]),out_size], stddev=np.sqrt(1/out_size)))\n",
    "    l = tf.constant(inputs.shape.as_list()[1],dtype=tf.float32)\n",
    "    mean_x = tf.reduce_mean(inputs, axis=1, keep_dims=True)\n",
    "    mean_w = tf.reduce_mean(w, axis=0, keep_dims=True)\n",
    "    hout = (2. / l) * tf.matmul(inputs, w) - mean_w - mean_x\n",
    "    return double_thresholding(hout)\n",
    "\n",
    "class Model:\n",
    "    def __init__(self,learning_rate):\n",
    "        self.X = tf.placeholder(tf.float32,shape=[None,28,28,1])\n",
    "        self.Y = tf.placeholder(tf.float32,shape=[None,10])\n",
    "        conv1 = tf.nn.relu(conv(self.X, 16, 5))\n",
    "        pool1 = tf.layers.max_pooling2d(conv1, [2, 2], [2, 2])\n",
    "        conv2 = tf.nn.relu(conv(pool1, 64, 5))\n",
    "        pool2 = tf.layers.max_pooling2d(conv2, [2, 2], [2, 2])\n",
    "        pool2_shape = pool2.shape.as_list()\n",
    "        pulled_pool2 = tf.reshape(pool2, [-1,pool2_shape[1] * pool2_shape[2] * pool2_shape[3]])\n",
    "        fc1 = tf.nn.relu(fully_connected(pulled_pool2,1024))\n",
    "        self.logits = fully_connected(fc1,10)\n",
    "        self.cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels = self.Y, logits = self.logits))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate).minimize(self.cost)\n",
    "        correct_prediction = tf.equal(tf.argmax(self.logits, 1), tf.argmax(self.Y, 1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "learning_rate = 0.2\n",
    "epoch = 10\n",
    "\n",
    "train_images = mnist.train.images.reshape((-1,28,28,1))\n",
    "test_images = mnist.test.images.reshape((-1,28,28,1))\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "model = Model(learning_rate)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, accuracy train: 0.108340, accuracy testing: 0.106027\n",
      "epoch: 2, accuracy train: 0.153412, accuracy testing: 0.633929\n",
      "epoch: 3, accuracy train: 0.897119, accuracy testing: 0.941964\n",
      "epoch: 4, accuracy train: 0.958150, accuracy testing: 0.967634\n",
      "epoch: 5, accuracy train: 0.967031, accuracy testing: 0.954241\n",
      "epoch: 6, accuracy train: 0.971538, accuracy testing: 0.960938\n",
      "epoch: 7, accuracy train: 0.974493, accuracy testing: 0.964286\n",
      "epoch: 8, accuracy train: 0.976846, accuracy testing: 0.981027\n",
      "epoch: 9, accuracy train: 0.979100, accuracy testing: 0.977679\n",
      "epoch: 10, accuracy train: 0.980736, accuracy testing: 0.977679\n"
     ]
    }
   ],
   "source": [
    "LOSS, ACC_TRAIN, ACC_TEST = [], [], []\n",
    "for i in range(epoch):\n",
    "    total_loss, total_acc = 0, 0\n",
    "    for n in range(0, (mnist.train.images.shape[0] // batch_size) * batch_size, batch_size):\n",
    "        batch_x = train_images[n: n + batch_size,:,:,:]\n",
    "        batch_y = np.zeros((batch_size, 10))\n",
    "        batch_y[np.arange(batch_size),mnist.train.labels[n:n+batch_size]] = 1.0\n",
    "        cost, _ = sess.run([model.cost, model.optimizer], \n",
    "                           feed_dict = {model.X : batch_x, \n",
    "                                        model.Y : batch_y})\n",
    "        total_acc += sess.run(model.accuracy, \n",
    "                              feed_dict = {model.X : batch_x, \n",
    "                                           model.Y : batch_y})\n",
    "        total_loss += cost\n",
    "    total_loss /= (mnist.train.images.shape[0] // batch_size)\n",
    "    total_acc /= (mnist.train.images.shape[0] // batch_size)\n",
    "    ACC_TRAIN.append(total_acc)\n",
    "    total_acc = 0\n",
    "    for n in range(0, (mnist.test.images[:1000,:].shape[0] // batch_size) * batch_size, batch_size):\n",
    "        batch_x = test_images[n: n + batch_size,:,:,:]\n",
    "        batch_y = np.zeros((batch_size, 10))\n",
    "        batch_y[np.arange(batch_size),mnist.test.labels[n:n+batch_size]] = 1.0\n",
    "        total_acc += sess.run(model.accuracy, \n",
    "                              feed_dict = {model.X : batch_x, \n",
    "                                           model.Y : batch_y})\n",
    "    total_acc /= (mnist.test.images[:1000,:].shape[0] // batch_size)\n",
    "    ACC_TEST.append(total_acc)\n",
    "    print('epoch: %d, accuracy train: %f, accuracy testing: %f'%(i+1, ACC_TRAIN[-1],ACC_TEST[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
