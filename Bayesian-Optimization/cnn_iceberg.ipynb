{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "6cf9684f-771d-464c-9cf5-30bf73ea8f64",
    "_uuid": "0d99a3cf087b8bf9c32b3cf40829ea14e4174cf6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('../input/train.json')\n",
    "df.inc_angle = df.inc_angle.replace('na', 0)\n",
    "df.inc_angle = df.inc_angle.astype(float).fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_band1 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in df[\"band_1\"]])\n",
    "x_band2 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in df[\"band_2\"]])\n",
    "X_train = np.concatenate([x_band1[:, :, :, np.newaxis]\n",
    "                          , x_band2[:, :, :, np.newaxis]\n",
    "                         , ((x_band1+x_band1)/2)[:, :, :, np.newaxis]], axis=-1)\n",
    "X_angle_train = np.array(df.inc_angle)\n",
    "y_train = np.array(df[\"is_iceberg\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just take 100 dataset to do optimization\n",
    "# we assume this 100 able to generalize the whole dataset\n",
    "# if not enough, increase the number\n",
    "X_train = X_train[:100]\n",
    "y_train = y_train[:100]\n",
    "X_angle_train = X_angle_train[:100].reshape((-1, 1))\n",
    "y_onehot = np.zeros((y_train.shape[0], np.unique(y_train).shape[0]))\n",
    "for i in range(y_train.shape[0]):\n",
    "    y_onehot[i, y_train[i]] = 1.0\n",
    "    \n",
    "x_train, x_test, y_train, y_test, x_train_angle, x_test_angle = train_test_split(X_train, y_onehot, X_angle_train, test_size = 0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network(fully_conn_size, # wide size of fully connected layer\n",
    "                   len_layer_conv, # each conv layer included max pooling\n",
    "                   kernel_size, # kernel size for conv process\n",
    "                   learning_rate, # learning rate\n",
    "                   pooling_size, # kernel and stride size for pooling\n",
    "                   multiply, # constant to multiply for conv output\n",
    "                   dropout_rate, # dropout\n",
    "                   beta, # l2 norm discount\n",
    "                   activation,\n",
    "                   batch_normalization, \n",
    "                   batch_size = 20):\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    if activation == 0:\n",
    "        activation = tf.nn.sigmoid\n",
    "    elif activation == 1:\n",
    "        activation = tf.nn.tanh\n",
    "    else:\n",
    "        activation = tf.nn.relu\n",
    "    \n",
    "    def conv_layer(x, conv, out_shape):\n",
    "        w = tf.Variable(tf.truncated_normal([conv, conv, int(x.shape[3]), out_shape]))\n",
    "        b = tf.Variable(tf.truncated_normal([out_shape], stddev = 0.01))\n",
    "        return tf.nn.conv2d(x, w, [1, 1, 1, 1], padding = 'SAME') + b\n",
    "    \n",
    "    def fully_connected(x, out_shape):\n",
    "        w = tf.Variable(tf.truncated_normal([int(x.shape[1]), out_shape]))\n",
    "        b = tf.Variable(tf.truncated_normal([out_shape], stddev = 0.01))\n",
    "        return tf.matmul(x, w) + b\n",
    "    \n",
    "    def pooling(x, k = 2, stride = 2):\n",
    "        return tf.nn.max_pool(x, ksize = [1, k, k, 1], \n",
    "                              strides = [1, stride, stride, 1], \n",
    "                              padding = 'SAME')\n",
    "    \n",
    "    X_img = tf.placeholder(tf.float32, (None, 75, 75, 3))\n",
    "    X_angle = tf.placeholder(tf.float32, (None, 1))\n",
    "    Y = tf.placeholder(tf.float32, (None, y_onehot.shape[1]))\n",
    "    # for batch normalization\n",
    "    train = tf.placeholder(tf.bool)\n",
    "    \n",
    "    for i in range(len_layer_conv):\n",
    "        if i == 0:\n",
    "            conv = activation(conv_layer(X_img, kernel_size, \n",
    "                                         int(np.around(int(X_img.shape[3]) * multiply))))\n",
    "        else:\n",
    "            conv = activation(conv_layer(conv, kernel_size, \n",
    "                                         int(np.around(int(conv.shape[3]) * multiply))))\n",
    "        conv = pooling(conv, k = pooling_size, stride = pooling_size)\n",
    "        if batch_normalization:\n",
    "            conv = tf.layers.batch_normalization(conv, training = train)\n",
    "        conv = tf.nn.dropout(conv, dropout_rate)\n",
    "    print(conv.shape)\n",
    "    output_shape = int(conv.shape[1]) * int(conv.shape[2]) * int(conv.shape[3])\n",
    "    conv = tf.reshape(conv, [-1, output_shape])\n",
    "    conv = tf.concat([conv, X_angle], axis = 1)\n",
    "    \n",
    "    # our fully connected got 1 layer\n",
    "    # you can increase it if you want\n",
    "    for i in range(1):\n",
    "        if i == 0:\n",
    "            fc = activation(fully_connected(conv, fully_conn_size))\n",
    "        else:\n",
    "            fc = activation(fully_connected(fc, fully_conn_size))\n",
    "        fc = tf.nn.dropout(fc, dropout_rate)\n",
    "        if batch_normalization:\n",
    "            fc = tf.layers.batch_normalization(fc, training = train)\n",
    "            \n",
    "    logits = fully_connected(fc, y_onehot.shape[1])\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = Y, \n",
    "                                                                  logits = logits))\n",
    "    cost += sum(beta * tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables())\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    \n",
    "    sess = tf.InteractiveSession()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    COST, TEST_COST, ACC, TEST_ACC = [], [], [], []\n",
    "    \n",
    "    for i in range(20):\n",
    "        train_acc, train_loss = 0, 0\n",
    "        for n in range(0, (X_train.shape[0] // batch_size) * batch_size, batch_size):\n",
    "            _, loss = sess.run([optimizer, cost], \n",
    "                               feed_dict = {X_img: x_train[n: n + batch_size, :, :, :],\n",
    "                                            X_angle: x_train_angle[n: n + batch_size],\n",
    "                                            Y: y_train[n: n + batch_size, :], \n",
    "                                            train: True})\n",
    "            train_acc += sess.run(accuracy, \n",
    "                                  feed_dict = {X_img: x_train[n: n + batch_size, :, :, :],\n",
    "                                               X_angle: x_train_angle[n: n + batch_size],\n",
    "                                               Y: y_train[n: n + batch_size, :], \n",
    "                                               train: False})\n",
    "            train_loss += loss\n",
    "        results = sess.run([cost, accuracy], \n",
    "                           feed_dict = {X_img: x_test,\n",
    "                                        X_angle: x_test_angle,\n",
    "                                        Y: y_test, \n",
    "                                        train: False})\n",
    "        TEST_COST.append(results[0])\n",
    "        TEST_ACC.append(results[1])\n",
    "        train_loss /= (x_train.shape[0] // batch_size)\n",
    "        train_acc /= (x_train.shape[0] // batch_size)\n",
    "        ACC.append(train_acc)\n",
    "        COST.append(train_loss)\n",
    "    COST = np.array(COST).mean()\n",
    "    TEST_COST = np.array(TEST_COST).mean()\n",
    "    ACC = np.array(ACC).mean()\n",
    "    TEST_ACC = np.array(TEST_ACC).mean()\n",
    "    return COST, TEST_COST, ACC, TEST_ACC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_nn(fully_conn_size, len_layer_conv, kernel_size, learning_rate, pooling_size, multiply,\n",
    "                dropout_rate, beta, activation, batch_normalization):\n",
    "    global accbest\n",
    "    param = {\n",
    "        'fully_conn_size' : int(np.around(fully_conn_size)),\n",
    "        'len_layer_conv' : int(np.around(len_layer_conv)),\n",
    "        'kernel_size': int(np.around(kernel_size)),\n",
    "        'learning_rate' : max(min(learning_rate, 1), 0.0001),\n",
    "        'pooling_size': int(np.around(pooling_size)),\n",
    "        'multiply': multiply,\n",
    "        'dropout_rate' : max(min(dropout_rate, 0.99), 0),\n",
    "        'beta' : max(min(beta, 0.5), 0.000001),\n",
    "        'activation': int(np.around(activation)),\n",
    "        'batch_normalization' : int(np.around(batch_normalization))\n",
    "    }\n",
    "    learning_cost, valid_cost, learning_acc, valid_acc = neural_network(**param)\n",
    "    print(\"stop after 20 iteration with train cost %f, valid cost %f, train acc %f, valid acc %f\" % (learning_cost, valid_cost, learning_acc, valid_acc))\n",
    "    # a very simple benchmark, just use correct accuracy\n",
    "    # if you want to change to f1 score or anything else, can\n",
    "    if (valid_acc > accbest):\n",
    "        costbest = valid_acc\n",
    "    return valid_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   activation |   batch_normalization |      beta |   dropout_rate |   fully_conn_size |   kernel_size |   learning_rate |   len_layer_conv |   multiply |   pooling_size | \n",
      "(?, 2, 2, 3)\n",
      "stop after 20 iteration with train cost nan, valid cost 12.430384, train acc nan, valid acc 0.600000\n",
      "    1 | 00m11s | \u001b[35m   0.60000\u001b[0m | \u001b[32m      0.7829\u001b[0m | \u001b[32m               0.4417\u001b[0m | \u001b[32m   0.4673\u001b[0m | \u001b[32m        0.7876\u001b[0m | \u001b[32m          45.7452\u001b[0m | \u001b[32m       5.7427\u001b[0m | \u001b[32m         0.5704\u001b[0m | \u001b[32m          3.1465\u001b[0m | \u001b[32m    1.0553\u001b[0m | \u001b[32m        3.8126\u001b[0m | \n",
      "(?, 3, 3, 36)\n",
      "stop after 20 iteration with train cost nan, valid cost nan, train acc nan, valid acc 0.400000\n",
      "    2 | 00m18s |    0.40000 |       1.2282 |                0.8450 |    0.0673 |         0.2630 |           16.0480 |        6.4503 |          0.4779 |           3.2515 |     2.2198 |         2.7465 | \n",
      "(?, 1, 1, 221)\n",
      "stop after 20 iteration with train cost nan, valid cost nan, train acc nan, valid acc 0.400000\n",
      "    3 | 00m16s |    0.40000 |       0.1120 |                0.6980 |    0.3254 |         0.4960 |           34.5135 |        5.2485 |          0.7449 |           4.3817 |     2.9078 |         3.7427 | \n",
      "(?, 1, 1, 143)\n",
      "stop after 20 iteration with train cost nan, valid cost nan, train acc nan, valid acc 0.400000\n",
      "    4 | 00m28s |    0.40000 |       0.9064 |                0.5437 |    0.2239 |         0.1009 |          110.0785 |        6.6746 |          0.1047 |           4.1076 |     2.5975 |         2.8028 | \n",
      "(?, 1, 1, 48)\n",
      "stop after 20 iteration with train cost nan, valid cost nan, train acc nan, valid acc 0.400000\n",
      "    5 | 00m03s |    0.40000 |       1.7474 |                0.0067 |    0.0440 |         0.8174 |          116.6944 |        2.3211 |          0.9795 |           3.9498 |     2.0026 |         3.7250 | \n",
      "(?, 1, 1, 3)\n",
      "stop after 20 iteration with train cost nan, valid cost 11.912154, train acc nan, valid acc 0.600000\n",
      "    6 | 00m05s |    0.60000 |       0.7277 |                0.3909 |    0.2616 |         0.6071 |           91.6583 |        2.8332 |          0.2810 |           3.8482 |     1.1559 |         3.0747 | \n",
      "(?, 5, 5, 173)\n",
      "stop after 20 iteration with train cost nan, valid cost 55609.699219, train acc nan, valid acc 0.592500\n",
      "    7 | 00m31s |    0.59250 |       0.7879 |                0.1073 |    0.4592 |         0.2502 |           67.7937 |        4.7486 |          0.2690 |           4.3562 |     2.7959 |         2.1530 | \n",
      "(?, 1, 1, 3)\n",
      "stop after 20 iteration with train cost nan, valid cost nan, train acc nan, valid acc 0.400000\n",
      "    8 | 00m16s |    0.40000 |       1.9890 |                0.2757 |    0.0399 |         0.2489 |           80.9954 |        6.6404 |          0.4286 |           3.5234 |     1.0856 |         3.4482 | \n",
      "(?, 1, 1, 89)\n",
      "stop after 20 iteration with train cost nan, valid cost nan, train acc nan, valid acc 0.400000\n",
      "    9 | 00m06s |    0.40000 |       1.2205 |                0.9488 |    0.4696 |         0.4479 |           37.9253 |        2.3696 |          0.5134 |           3.5527 |     2.3449 |         3.9133 | \n",
      "(?, 1, 1, 89)\n",
      "stop after 20 iteration with train cost nan, valid cost nan, train acc nan, valid acc 0.400000\n",
      "   10 | 00m16s |    0.40000 |       0.0551 |                0.5532 |    0.4881 |         0.4577 |           66.5911 |        4.6565 |          0.6994 |           3.8198 |     2.3507 |         2.6258 | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   activation |   batch_normalization |      beta |   dropout_rate |   fully_conn_size |   kernel_size |   learning_rate |   len_layer_conv |   multiply |   pooling_size | \n",
      "(?, 3, 3, 729)\n",
      "stop after 20 iteration with train cost nan, valid cost nan, train acc nan, valid acc 0.400000\n",
      "   11 | 02m09s |    0.40000 |       2.0000 |                0.0000 |    0.4900 |         0.1000 |           73.3309 |        2.0000 |          0.0001 |           5.0000 |     3.0000 |         2.0000 | \n",
      "(?, 3, 3, 729)\n",
      "stop after 20 iteration with train cost nan, valid cost nan, train acc nan, valid acc 0.400000\n",
      "   12 | 03m35s |    0.40000 |       2.0000 |                0.0000 |    0.0000 |         0.1000 |           68.2941 |        7.0000 |          0.0001 |           5.0000 |     3.0000 |         2.0000 | \n",
      "(?, 2, 2, 81)\n",
      "stop after 20 iteration with train cost nan, valid cost nan, train acc nan, valid acc 0.400000\n",
      "   13 | 00m58s |    0.40000 |       2.0000 |                0.0000 |    0.0000 |         0.9900 |           97.3657 |        7.0000 |          1.0000 |           3.0000 |     3.0000 |         4.0000 | \n",
      "(?, 3, 3, 729)\n",
      "stop after 20 iteration with train cost nan, valid cost nan, train acc nan, valid acc 0.400000\n",
      "   14 | 03m40s |    0.40000 |       0.0000 |                1.0000 |    0.4900 |         0.1000 |          128.0000 |        7.0000 |          0.0001 |           5.0000 |     3.0000 |         2.0000 | \n",
      "(?, 3, 3, 729)\n",
      "stop after 20 iteration with train cost nan, valid cost nan, train acc nan, valid acc 0.400000\n",
      "   15 | 01m06s |    0.40000 |       2.0000 |                0.0000 |    0.4900 |         0.1000 |           51.1080 |        2.0000 |          0.0001 |           5.0000 |     3.0000 |         2.0000 | \n",
      "(?, 3, 3, 729)\n",
      "stop after 20 iteration with train cost nan, valid cost nan, train acc nan, valid acc 0.400000\n",
      "   16 | 01m05s |    0.40000 |       2.0000 |                0.0000 |    0.4900 |         0.1000 |           67.1000 |        2.0000 |          0.0001 |           5.0000 |     3.0000 |         2.0000 | \n",
      "(?, 10, 10, 81)\n",
      "stop after 20 iteration with train cost nan, valid cost 120134.359375, train acc nan, valid acc 0.490000\n",
      "   17 | 02m17s |    0.49000 |       0.0000 |                0.0000 |    0.4900 |         0.1000 |           70.7609 |        4.6815 |          0.0001 |           3.0000 |     3.0000 |         2.0000 | \n",
      "(?, 3, 3, 729)\n",
      "stop after 20 iteration with train cost nan, valid cost 120.540894, train acc nan, valid acc 0.590000\n",
      "   18 | 05m01s |    0.59000 |       0.0000 |                0.0000 |    0.4900 |         0.9900 |           88.2054 |        7.0000 |          1.0000 |           5.0000 |     3.0000 |         2.0000 | \n",
      "(?, 3, 3, 729)\n",
      "stop after 20 iteration with train cost nan, valid cost nan, train acc nan, valid acc 0.400000\n",
      "   19 | 05m01s |    0.40000 |       2.0000 |                0.0000 |    0.4900 |         0.1000 |           43.3221 |        7.0000 |          0.0001 |           5.0000 |     3.0000 |         2.0000 | \n",
      "(?, 2, 2, 3)\n",
      "stop after 20 iteration with train cost nan, valid cost 0.734473, train acc nan, valid acc 0.600000\n",
      "   20 | 02m08s |    0.60000 |       0.0000 |                0.0000 |    0.4900 |         0.9900 |           24.3525 |        2.0000 |          1.0000 |           3.0000 |     1.0000 |         4.0000 | \n"
     ]
    }
   ],
   "source": [
    "accbest = 0.0\n",
    "NN_BAYESIAN = BayesianOptimization(generate_nn, \n",
    "                              {'fully_conn_size': (16, 128),\n",
    "                               'len_layer_conv': (3, 5),\n",
    "                               'kernel_size': (2, 7),\n",
    "                               'learning_rate': (0.0001, 1),\n",
    "                               'pooling_size': (2, 4),\n",
    "                               'multiply': (1, 3),\n",
    "                               'dropout_rate': (0.1, 0.99),\n",
    "                               'beta': (0.000001, 0.49),\n",
    "                               'activation': (0, 2),\n",
    "                               'batch_normalization': (0, 1)\n",
    "                              })\n",
    "NN_BAYESIAN.maximize(init_points = 10, n_iter = 10, acq = 'ei', xi = 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum NN accuracy value: 0.600000\n",
      "Best NN parameters:  {'fully_conn_size': 45.745233745490474, 'len_layer_conv': 3.1465432539809788, 'kernel_size': 5.7426796270733682, 'learning_rate': 0.57043391227406293, 'pooling_size': 3.8125736787655145, 'multiply': 1.0552589410978261, 'dropout_rate': 0.78762606131084778, 'beta': 0.46732759735648771, 'activation': 0.78293147230772098, 'batch_normalization': 0.44170041567714113}\n"
     ]
    }
   ],
   "source": [
    "print('Maximum NN accuracy value: %f' % NN_BAYESIAN.res['max']['max_val'])\n",
    "print('Best NN parameters: ', NN_BAYESIAN.res['max']['max_params'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is low because our cnn model is very complex. the purpose is, bayesian still able to find the best maxima in non convex hyper-parameters function without do any derivation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
