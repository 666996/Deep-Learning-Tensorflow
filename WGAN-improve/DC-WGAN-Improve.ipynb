{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can read the paper [here](https://arxiv.org/pdf/1704.00028.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://raw.githubusercontent.com/huseinzol05/Deep-Learning-Tensorflow/master/WGAN-improve/wgan.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import misc\n",
    "sns.set()\n",
    "face_path = '/home/husein/space/facedataset/img_align_celeba/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('/home/husein/space/discogan/desc.csv')\n",
    "dataset.head()\n",
    "print dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scale(x):\n",
    "    return x / 127.5 - 1\n",
    "\n",
    "def originate(x):\n",
    "    return (x + 1.) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_sample(samples):\n",
    "    idx = [i for i in xrange(16)]\n",
    "    fig, axes = plt.subplots(4, 4, sharex = True, sharey = True, figsize = (5,5))\n",
    "\n",
    "    for ii, ax in zip(idx, axes.flatten()):\n",
    "        ax.imshow(originate(samples[ii,:,:,:]), aspect = 'equal')\n",
    "        ax.xaxis.set_visible(False)\n",
    "        ax.yaxis.set_visible(False)\n",
    "\n",
    "    plt.subplots_adjust(wspace = 0, hspace = 0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator(z, name, reuse = False, alpha = 0.2, training = True):\n",
    "    with tf.variable_scope(name, reuse = reuse):\n",
    "        \n",
    "        x1 = tf.layers.dense(z, 4 * 4 * 1024)\n",
    "        x1 = tf.reshape(x1, (-1, 4, 4, 1024))\n",
    "        x1 = tf.layers.batch_normalization(x1, training = training)\n",
    "        x1 = tf.maximum(alpha * x1, x1)\n",
    "                \n",
    "        x2 = tf.layers.conv2d_transpose(x1, 512, 5, strides = 2, padding = 'same')\n",
    "        bn2 = tf.layers.batch_normalization(x2, training = training)\n",
    "        relu2 = tf.maximum(alpha * bn2, bn2)\n",
    "                \n",
    "        x3 = tf.layers.conv2d_transpose(relu2, 256, 5, strides = 2, padding = 'same')\n",
    "        bn3 = tf.layers.batch_normalization(x3, training = training)\n",
    "        relu3 = tf.maximum(alpha * bn3, bn3)\n",
    "                \n",
    "        x4 = tf.layers.conv2d_transpose(relu3, 128, 5, strides = 2, padding = 'same')\n",
    "        bn4 = tf.layers.batch_normalization(x4, training = training)\n",
    "        relu4 = tf.maximum(alpha * bn4, bn4)\n",
    "                \n",
    "        x5 = tf.layers.conv2d_transpose(relu4, 3, 5, strides = 2, padding = 'same')\n",
    "                \n",
    "        return tf.nn.tanh(x5)\n",
    "                \n",
    "def discriminator(z, name, reuse = False, alpha = 0.2):\n",
    "    with tf.variable_scope(name, reuse = reuse):\n",
    "        x1 = tf.layers.conv2d(z, 64, 5, strides = 2, padding = 'same')\n",
    "        relu1 = tf.maximum(alpha * x1, x1)\n",
    "                \n",
    "        x2 = tf.layers.conv2d(relu1, 128, 5, strides = 2, padding = 'same')\n",
    "        bn2 = tf.layers.batch_normalization(x2, training = True)\n",
    "        relu2 = tf.maximum(alpha * bn2, bn2)\n",
    "                \n",
    "        x3 = tf.layers.conv2d(relu2, 256, 5, strides = 2, padding = 'same')\n",
    "        bn3 = tf.layers.batch_normalization(x3, training = True)\n",
    "        relu3 = tf.maximum(alpha * bn3, bn3)\n",
    "                \n",
    "        x4 = tf.layers.conv2d(relu3, 512, 5, strides = 2, padding = 'same')\n",
    "        bn4 = tf.layers.batch_normalization(x4, training = True)\n",
    "        relu4 = tf.maximum(alpha * bn4, bn4)\n",
    "        \n",
    "        flat = tf.reshape(relu4, (-1, 4 * 4 * 512))\n",
    "        logits = tf.layers.dense(flat, 1)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class WGAN:\n",
    "    \n",
    "    def __init__(self, batch_size = 128, learning_rate = 0.0002, alpha = 0.2, LAMBDA = 10):\n",
    "        self.X = tf.placeholder(tf.float32, (None, 100))\n",
    "        self.Y = tf.placeholder(tf.float32, (None, 64, 64, 3))\n",
    "        \n",
    "        g_model = generator(self.X, 'generator', alpha = alpha)\n",
    "        self.g_out = generator(self.X, 'generator', reuse = True, training = False)\n",
    "        d_logits_real = discriminator(self.Y, 'discriminator', alpha = alpha)\n",
    "        d_logits_fake = discriminator(g_model, 'discriminator', reuse = True, alpha = alpha)\n",
    "        \n",
    "        self.g_loss = -tf.reduce_mean(d_logits_fake)\n",
    "        self.d_loss = tf.reduce_mean(d_logits_fake) - tf.reduce_mean(d_logits_real)\n",
    "        \n",
    "        alpha = tf.random_uniform(shape = [batch_size, 1], minval = 0., maxval = 1.)\n",
    "        differences = g_model - self.Y\n",
    "        interpolates = self.Y + (alpha * differences)\n",
    "        gradients = tf.gradients(discriminator(interpolates, 'discriminator', reuse = True, alpha = alpha), [interpolates])[0]\n",
    "        slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients), reduction_indices = [1]))\n",
    "        gradient_penalty = tf.reduce_mean((slopes - 1.) ** 2)\n",
    "        self.d_loss += LAMBDA * gradient_penalty\n",
    "        \n",
    "        t_vars = tf.trainable_variables()\n",
    "        d_vars = [var for var in t_vars if var.name.startswith('discriminator')]\n",
    "        g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "\n",
    "        self.d_train_opt = tf.train.AdamOptimizer(learning_rate, beta1 = 0.5, beta2 = 0.999).minimize(self.d_loss, var_list = d_vars)\n",
    "        self.g_train_opt = tf.train.AdamOptimizer(learning_rate, beta1 = 0.5, beta2 = 0.999).minimize(self.g_loss, var_list = g_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, dataset, batch, epoch):\n",
    "    sample_z = np.random.uniform(-1, 1, size = (72, 100))\n",
    "    LOSS_D, LOSS_G = [], []\n",
    "    for i in xrange(epoch):\n",
    "        g_loss, d_loss = 0, 0\n",
    "        for k in xrange(0, (dataset.shape[0] // batch) * batch, batch):\n",
    "            batch_x = np.random.uniform(-1, 1, size = (batch, 100))\n",
    "            batch_y = np.zeros((batch, 64, 64, 3), dtype = np.float32)\n",
    "            for n in xrange(batch):\n",
    "                img_y = misc.imread(face_path + dataset.iloc[k + n, 0])\n",
    "                img_y = misc.imresize(img_y, (64, 64))\n",
    "                batch_y[n, :, :, :] = scale(img_y)\n",
    "            _, lossd = sess.run([model.d_train_opt, model.d_loss], feed_dict = {model.X: batch_x, model.Y: batch_y})\n",
    "            _, lossg = sess.run([model.g_train_opt, model.g_loss], feed_dict = {model.X: batch_x, model.Y: batch_y})\n",
    "            g_loss += lossg; d_loss += lossd\n",
    "        g_loss /= (dataset.shape[0] // batch); d_loss /= (dataset.shape[0] // batch)\n",
    "        \n",
    "        print(\"Epoch {}/{}...\".format(i + 1, EPOCH), \"Discriminator Loss: {}\".format(d_loss), \"Generator Loss: {}\".format(g_loss))\n",
    "        LOSS_G.append(g_loss); LOSS_D.append(d_loss)\n",
    "        \n",
    "        outputs = sess.run(model.g_out, feed_dict = {model.X: sample_z})\n",
    "        generate_sample(outputs)\n",
    "    \n",
    "    epoch = [i for i in xrange(len(LOSS_D))]\n",
    "    plt.plot(epoch,LOSS_D, label = 'Discriminator', alpha = 0.5)\n",
    "    plt.plot(epoch, LOSS_G, label = 'Generator', alpha = 0.5)\n",
    "    plt.title(\"Training Losses\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EPOCH = 10\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "model = WGAN()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "train(model, dataset, BATCH_SIZE, EPOCH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
